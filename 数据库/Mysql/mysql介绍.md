
[toc]

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsclipboard.png)

# JDBC过程

1. 加载驱动
2. 建立连接
3. 创建preparedStatement    
4. 执行sql并且遍历返回
5. 关闭

# 关系型数据库和非关系型数据库

## 关系型数据库
关系型数据库：指采用了关系模型来组织数据的数据库。关系模型指的就是二维表格模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。

比如：Oracle，Microsoft SQL Server，MySQL，PostgreSQL，DB2，
Microsoft Access， SQLite，Teradata，MariaDB(MySQL的一个分支)，SAP
### 优点：
1. 容易理解：二维表结构是非常贴近逻辑世界的一个概念，关系模型相对网状、层次等其他模型来说更容易理解
2. 使用方便：通用的SQL语言使得操作关系型数据库非常方便
3. 易于维护：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率
4. 数据库事务必须具备ACID特性，ACID分别是Atomic原子性，Consistency一致性，
Isolation隔离性，Durability持久性。




## 非关系型数据库
非关系型数据库：指非关系型的，分布式的，且一般不保证遵循ACID原则的数据存储系统。非关系型数据库以键值对存储，且结构不固定，每一个元组可以有不一样的字段，每个元组可以根据需要增加一些自己的键值对，不局限于固定的结构，可以减少一些时间和空间的开销。

### 分类
1. 面向高性能并发读写的key-value数据库：
key-value数据库的主要特点是具有极高的并发读写性能

    Key-value数据库是一种以键值对存储数据的一种数据库，类似Java中的map。可以将整个数据库理解为一个大的map，每个键都会对应一个唯一的值。
    
    主流代表为Redis， Amazon DynamoDB， Memcached，Microsoft Azure Cosmos DB和Hazelcast

2. 面向海量数据访问的面向文档数据库：
这类数据库的主要特点是在海量的数据中可以快速的查询数据
文档存储通常使用内部表示法，可以直接在应用程序中处理，主要是JSON。JSON文档也可以作为纯文本存储在键值存储或关系数据库系统中。
    
    主流代表为MongoDB，Amazon DynamoDB，Couchbase，Microsoft Azure Cosmos DB和CouchDB

3. 面向搜索数据内容的搜索引擎：
    搜索引擎是专门用于搜索数据内容的NoSQL数据库管理系统。
    主要是用于对海量数据进行近实时的处理和分析处理，可用于机器学习和数据挖掘
    
    主流代表为Elasticsearch，Splunk，Solr，MarkLogic和Sphinx

3. 面向可扩展性的分布式数据库：
    这类数据库的主要特点是具有很强的可拓展性
    普通的关系型数据库都是以行为单位来存储数据的，擅长以行为单位的读入处理，比如特定条件数据的获取。因此，关系型数据库也被成为面向行的数据库。相反，面向列的数据库是以列为单位来存储数据的，擅长以列为单位读入数据。
    这类数据库想解决的问题就是传统数据库存在可扩展性上的缺陷，这类数据库可以适应数据量的增加以及数据结构的变化，将数据存储在记录中，能够容纳大量动态列。由于列名和记录键不是固定的，并且由于记录可能有数十亿列，因此可扩展性存储可以看作是二维键值存储。

    主流代表为Cassandra，HBase，Microsoft Azure Cosmos DB，Datastax Enterprise和Accumulo



### 优点
1. 用户可以根据需要去添加自己需要的字段，为了获取用户的不同信息，不像关系型数据库中，要对多表进行关联查询。仅需要根据id取出相应的value就可以完成查询。
2. 适用于SNS(Social Networking Services)中，例如facebook，微博。系统的升级，功能的增加，往往意味着数据结构巨大变动，这一点关系型数据库难以应付，需要新的结构化数据存储。由于不可能用一种数据结构化存储应付所有的新的需求，因此，非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合。

### 不足：
只适合存储一些较为简单的数据，对于需要进行较复杂查询的数据，关系型数据库显的更为合适。不适合持久存储海量数据


## 缺点
1. 海量数据的读写效率。

    1. 网站的用户并发性非常高，往往达到每秒上万次读写请求，对于传统关系型数据库来说，硬盘I/O是一个很大的瓶颈
    2. 网站每天产生的数据量是巨大的，对于关系型数据库来说，在一张包含海量数据的表中查询，效率是非常低的
2. 高扩展性和可用性。

   1. 在基于web的结构中，数据库是最难以横向拓展的，当一个应用系统的用户量和访问量与日俱增的时候，数据库没有办法像web Server那样简单的通过添加更多的硬件和服务节点来拓展性能和负载能力。
    
    2. 性能欠佳：在关系型数据库中，导致性能欠佳的最主要原因是多表的关联查询，以及复杂的数据分析类型的复杂SQL报表查询。为了保证数据库的ACID特性，必须尽量按照其要求的范式进行设计，关系型数据库中的表都是存储一个格式化的数据结构。

## 对比
1. 储存：非关是key-value,关是以数据库表的形式
2. 事务：关如果更新多个表，其中一个失败则全失败，这种场景可以通过事务来控制，可以在所有命令完成之后，再统一提交事务。非关没有事务。
3. 数据表 VS 数据集：  关系型是表格型的，存储在数据表的行和列中。彼此关联，容易提取。而非关系型是大块存储在一起。
4. 预定义结构 VS 动态结构：关系型必须定义好地段和表结构之后，才能够添加数据，例如定义表的主键、索引、外键等。表结构可以在定义之后更新，但是如果有比较大的结构变更，就会变的比较复杂。非关系数据可以在任何时候任何地方添加。不需要预先定义。
5. 存储规范 VS 存储代码：关系型数据库为了规范性，把数据分配成为最小的逻辑表来存储避免重复，获得精简的空间利用。但是多个表之间的关系限制，多表管理就有点复杂。非关系型是平面数据集合中，数据经常可以重复，单个数据库很少被分开，而是存储成为一个整体，这种整块读取数据效率更高。
6. 纵向拓展 VS 横向拓展：了支持更多的并发量，关系型数据采用纵向扩展，提高处理能力，通过提高计算机性能来提高处理能力。非关系通过横向拓展，非关系型数据库天然是分布式的，所以可以通过集群来实现负载均衡。


## 数据库语句执行过程
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190610212816.png)

大体来说，MySQL可以分为Server层和存储引擎层两部分。

1. ==Server层包括连接器、查询缓存、分析器、优化器、执行器等==，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。还有一个通用的日志模块 binglog 日志模块。

2. ==存储引擎层负责数据的存储和提取==。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。也就是说，你执行create table建表的时候，如果不指定引擎类型，默认使用的就是InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在create table语句中使用engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。==其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。==

从图中不难看出，不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用：


- 连接器： 身份认证和权限相关(登录 MySQL 的时候)。
- 查询缓存: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。
- 分析器: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。
- 优化器： 按照 MySQL 认为最优的方案去执行。
- 执行器:  验证是否对表有权限操作，有执行语句，然后从存储引擎返回数据。无则抛出问题。

## 概念
- 数据库: 数据库是一些关联表的集合。
- 数据表: 表是数据的矩阵。在一个数据库中的表看起来像一个简单的电子表格。
- 列: 一列(数据元素) 包含了相同的数据, 例如邮政编码的数据。
- 行：一行（=元组，或记录）是一组相关的数据，例如一条用户订阅的数据。
- 冗余：存储两倍数据，冗余降低了性能，但提高了数据的安全性。
- 主键：主键是唯一的。一个数据表中只能包含一个主键。你可以使用主键来查询数据。
- 外键：外键用于关联两个表。
- 复合键：复合键（组合键）将多个列作为一个索引键，一般用于复合索引。
- 索引：使用索引可快速访问数据库表中的特定信息。索引是对数据库表中一列或多列的值进行排序的一种结构。类似于书籍的目录。
- 参照完整性: 参照的完整性要求关系中不允许引用不存在的实体。与实体完整性是关系模型必须满足的完整性约束条件，目的是保证数据的一致性。
- 表头(header): 每一列的名称;
- 列(col): 具有相同数据类型的数据的集合;
- 行(row): 每一行用来描述某条记录的具体信息;
- 值(value): 行的具体信息, 每个值必须与该列的数据类型相同;
- 键(key): 键的值在当前列中具有唯一性。


数据库设计的基本步骤： 
1. 需求分析阶段： 
2. 概念结构设计阶段： 是整个数据库设计的关键,通过对用户的需求进行综合、归纳与抽象,形成一个独立于具体DBMS的概念模型.从实际到理论. 
3. 逻辑结构设计阶段： 将概念结构转换为某个DBMS所支持的数据模型,对其进行优化.优化理论. 
4. 数据库物理设计阶段： 为逻辑数据模型选取一个最适合应用环境的物理结构（包括存储结构和存取方法）.选择理论落脚点. 
5. 数据库实施阶段： 运用DBMS提供的数据语言、工具及宿主语言,根据逻辑设计和物理设计的结果,建立数据库,编制与调试应用程序,组织数据入库,并进行试运行.理论应用于实践. 
6. 数据库运行和维护阶段： 数据库应用系统经过试运行后即可投入正式运行.在数据库系统运行过程中必须不断地对其进行评价、调整与修改.理论指导实践,反过来实践修正理论.

## SQL语句分为哪几类？
- DDL(Data Definition Language)：数据定义语言，定义对数据库对象(库、表、列、索引)的操作。代表指令：CREATE、DROP、ALTER、RENAME、 TRUNCATE等

- DML(Data Manipulation Language)：数据操作语言，定义对数据库记录的操作。代表指令：INSERT、DELETE、UPDATE、SELECT等

- DCL(Data Control Language)：数据控制语言，定义对数据库、表、字段、用户的访问权限和安全级别。代表指令：GRANT、REVOKE，COMMIT、ROLLBACK、SAVEPOINT等

1. DDL：数据库模式定义语言，关键字：create
1. DML：数据操纵语言，关键字：Insert、delete、update、select
1. DCL：数据库控制语言 ，关键字：grant、revoke、savepoint


## 日志
### 重要的日志模块：redo log
redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。Redo log不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。


如果一家酒店有赊账还账的话，一般有两种做法：

- 一种做法是直接把原来的账本翻出来，把这次更新加上去；
- 另一种做法是先在粉板方记下这次的账，等空闲后以后再把账本翻出来核算。

酒店如果更新频繁，那么肯定会选择后者，毕竟每次都要取经过查找，会导致效率变低。在MySQL里同样的道理，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。

>具体来说，当有一条记录需要更新的时候：
1. InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。

2. 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。与此类似，InnoDB的redo log是固定大小的，==比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。==

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190610220224.png)
- write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

- write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。

- 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。

### 重要的日志模块：binlog
前面我们讲过，MySQL整体来看，其实就有两块：==一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜==。上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。

sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。==Binlog有两种模式，statement 格式的话是记sql语句，row格式会记录行的内容，记两条，更新前和更新后都有。==

>我想你肯定会问，为什么会有两份日志呢？

因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。

这两种日志有以下三点不同。

==其核心就是， redo log 记录的，即使异常重启，都会刷新到磁盘，而 bin log 记录的， 则主要用于备份。==
- redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
- redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
- redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
1. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
1. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。
1. 执行器生成这个操作的binlog，并把binlog写入磁盘。
1. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。

最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤：prepare和commit，这就是"两阶段提交"。
#### 两阶段提交
为什么必须有“两阶段提交”呢？==这是为了让两份日志之间的逻辑一致==。

>要说明这个问题，我们得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？

前面我们说过了，==binlog会记录所有的逻辑操作，并且是采用“追加写”的形式==。如果你的DBA承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

1. 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
2. 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。

由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

>仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？

- 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。
但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。
然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。

- 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。

可以看到，==如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致==。你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？

其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用binlog来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。

简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

这里我给出这个update语句的执行流程图，图中==浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的==。![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190610224321.png)

如果在图中时刻A的地方，也就是写入redo log 处于prepare阶段之后、写binlog之前，发生了崩溃（crash），由于此时binlog还没写，redo log也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog还没写，所以也不会传到备库。

大家出现问题的地方，主要集中在时刻B，也就是binlog写完，redo log还没commit前发生crash，那崩溃恢复的时候MySQL会怎么处理？我们先来看一下崩溃恢复时的判断规则。

- 如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交；

-  如果redo log里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整：
    - a. 如果是，则提交事务；
    - b. 否则，回滚事务。

这里，时刻B发生crash对应的就是2(a)的情况，崩溃恢复过程中事务会被提交。

>MySQL怎么知道binlog是完整的?
回答：一个事务的binlog是有完整格式的：
- statement格式的binlog，最后会有COMMIT；
- row格式的binlog，最后会有一个XID event。

另外，在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验checksum的结果来发现。所以，MySQL还是有办法验证事务binlog的完整性的。

>redo log 和 binlog是怎么关联起来的?
回答：它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：

- 如果碰到既有prepare、又有commit的redo log，就直接提交；
- 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。

>处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计?
回答：其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻B，也就是binlog写完以后MySQL发生崩溃，这时候binlog已经写入了，之后就会被从库（或者用这个binlog恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

>如果这样的话，为什么还要两阶段提交呢？干脆先redo log写完，再写binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？
回答：其实，两阶段提交是经典的分布式系统问题，并不是MySQL独有的。

如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。

对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。

两阶段提交就是为了给所有人一个机会，当每个人都说“我ok”的时候，再一起提交。


## 操作和语法
1. select 查询列表 7 
1. from 表1 别名 1 
1. 连接类型 join 表2 2 
1. on 连接条件 3 
1. where 筛选 4 
1. group by 分组列表 5 
1. having 筛选 6 
1. order by 排序列表 8 
1. limit 起始条目索引，条目数 9

```
from > on> join > where >  group by > with > having > select > distinct  > order by  > limit
```
COUNT ( * )不忽略空值  (null)；
### where 和 group by 和 having
1. having只能用在group by之后，对分组后的结果进行筛选，筛选行(即使用having的前提条件是分组)。
2. where肯定在group by 之前
3. where后的条件表达式里不允许使用聚合函数，而having可以。
4. group by 通常和集合函数SUM(),AVG().MAX(),MIN(),COUNT()等结合在一起，后接限制条件语句 having,不可用where语句！


```
select id,count(course),sum(score)
from score
group by id  
having sum(score) > 200
```



### drop、delete与truncate的区别

三者都表示删除，但是三者有一些差别：

|          |                  Delete                  |            Truncate            |                         Drop                         |
| :------: | :--------------------------------------: | :----------------------------: | :--------------------------------------------------: |
|   类型   |                 属于DML                  |            属于DDL             |                       属于DDL                        |
|   回滚   |                  可回滚                  |            不可回滚            |                       不可回滚                       |
| 删除内容 | 表结构还在，删除表的全部或者一部分数据行 | 表结构还在，删除表中的所有数据 | 从数据库中删除表，所有的数据行，索引和权限也会被删除 |
| 删除速度 |         删除速度慢,需要逐行删除          |           删除速度快           |                      删除速度快                      |

因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。

### distinct

一般是用来去除查询结果中的重复记录的，而且这个语句在select、insert、delete和update中只可以在select中使用，

```
select distinct expression[,expression...] 
from tables 
where conditions;
```

### 链接JOIN

避免使用子查询，可以把子查询优化为 join 操作
通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。

子查询性能差的原因：

子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。

由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。

==避免使用 JOIN 关联太多的表==

在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。

如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。

同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。


- 自然连接:必须至少存在一个完全的属性。找出属性属性值相同的部分，然后合并两个表。
- 外连接：将自然连接中剩下的部分（没有被匹配上的），也加进来（不存在的属性填NULL）
- 左外连接，将外连接的内容和左边的（如这里的R）进行比较，在R中的(R,null)留下来，在S中的出来;右外连接同理


#### INNER JOIN（内连接）

内连接是一种一一映射关系，就是两张表都有的才能显示出来



![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190617210649.png)



```
SELECT  A.PK AS A_PK,A.Value AS A_Value,B.PK AS B_PK,B.Value AS B_Value
FROM table_a A
INNER JOIN table_b B
ON A.PK = B.PK;
```

#### LEFT JOIN （左连接）

左连接是左边表的所有数据都有显示出来，右边的表数据只显示共同有的那部分，没有对应的部分只能补空显示，所谓的左边表其实就是指放在left join的左边的表


![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190617210707.png)

```
SELECT  A.PK AS A_PK,A.Value AS A_Value,B.PK AS B_PK,B.Value AS B_Value
FROM table_a A
LEFT JOIN table_b B
ON A.PK = B.PK;
```

#### RIGHT JOIN（右连接）

右连接正好是和左连接相反的，这里的右边也是相对right join来说的，在这个右边的表就是右表



![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190617210718.png)

```
SELECT  A.PK AS A_PK,A.Value AS A_Value,B.PK AS B_PK,B.Value AS B_Value
FROM table_a A
RIGHT JOIN table_b B
ON A.PK = B.PK;
```

#### OUTER JOIN（外连接、全连接）

查询出左表和右表所有数据，但是去除两表的重复数据



![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190617210726.png)

因为mysql不支持全连接，只能用以下代码实现效果，含义是左连接+右连接+去重=全连接：

```
SELECT  A.PK AS A_PK,A.Value AS A_Value,B.PK AS B_PK,B.Value AS B_Value
FROM table_a A
LEFT JOIN  table_b B
ON A.PK = B.PK
 UNION
SELECT  A.PK AS A_PK,A.Value AS A_Value,B.PK AS B_PK,B.Value AS B_Value
FROM table_a A
RIGHT JOIN  table_b B
ON A.PK = B.PK;
```
#### 笛卡尔积

A表a条数据
B表b条数据

那么检索起来的数据一共有a*b条数据

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/file/20190804164646.png)

解决办法
这样冗余的数据可不是我们想要，所以想要你的结果避免笛卡尔积，既要做到以下几点：

1.在使用条件的时候注意格式为:表名.列名=表名.列名
因为有时候两张表会出现同样的表名,会导致MySQL没法区分，加上表名调用列名就会不造成歧义

具体命令格式


```
SELECT  列名1, 列名2  FORM 表名1,表名2 WHERE 表名1.列名1=表名2.列名2
```


2.使用内连接
语法


```
Select 表1.*,表2.* ... From 表1   [Inner] Join 表2 on 表与表之间的关联

SELECT st.stuname 姓名, s.subjectname 科目名称,r.score   from  `subject` s JOIN result r on s.id=r.s_id

JOIN  student st  on   r.stuno=st.studentno
```


3.外连接
(1)左外连接


```
SELECT 字段…… FROM 表一 left [outer] JOIN 表2 ON 连接条件
```


当出现两个表中未匹配的信息，以左表为主（保证左表信息全部能够展示），右表中未匹配信息被省略

2.右外连接


```
SELECT 字段…… FROM 表一 RIGHT [outer] JOIN 表2 ON 连接条件
```


当出现两个表中未匹配的信息，以右表为主（保证右表信息全部能够展示），左表中未匹配信息被省略

#### 交叉连接

没有 WHERE 子句的交叉联接将产生联接所涉及的表的笛卡尔积。第一个表的行数乘以第二个表的行数等于笛卡尔积结果集的大小。
 用法：A CROSS JOIN B (不要ON)



[数据库左连接、右连接、内连接、全连接笔记](https://blog.csdn.net/u014204541/article/details/79739980)

### 交集 并集 差集
- union　并集
- expect 差集
- InterSect 交集

```
select_statement union [all] select_statement
select_statement except select_statement　
select_statement intersect select_statement　
```

## 数据类型

### 整型

TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。

INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。

FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。

FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190617132345.png)



### 字符串
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190617132447.png)
主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。

VARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。

在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。

### 时间和日期
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190617132507.png)
MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。

#### 1. DATETIME

能够保存从 1000 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。

它与时区无关。

默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22<span>:</span>37<span>:</span>08”，这是 ANSI 标准定义的日期和时间表示方法。

#### 2. TIMESTAMP

和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。

它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。

MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。

默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。

应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。

### 数据库字段设计规范
#### 优先选择符合存储需要的最小的数据：
类型列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。

方法：

1. 将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据


```
MySQL 提供了两个方法来处理 ip 地址
- inet_aton 把 ip 转为无符号整型 (4-8 位)
- inet_ntoa 把整型的 ip 转为地址
```
插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。

2. 对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储，无符号相对于有符号可以多出一倍的存储空间

VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。过大的长度会消耗更多的内存。
#### 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据

1. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中

MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。

如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。

2. TEXT 或 BLOB 类型只能使用前缀索引

因为MySQL[1] 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的
#### 同财务相关的金额类数据必须使用 decimal 类型
- 非精准浮点：float,double
- 精准浮点：decimal


#### 避免使用 ENUM 类型
修改 ENUM 值需要使用 ALTER 语句

ENUM 类型的 ORDER BY 操作效率低，需要额外操作

禁止使用数值作为 ENUM 的枚举值

#### 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间
TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07

TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高

超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储

经常会有人用字符串存储日期型的数据（不正确的做法）

- 缺点 1：无法用日期函数进行计算和比较
- 缺点 2：用字符串存储日期要占用更多的空间

Decimal 类型为精准浮点数，在计算时不会丢失精度

占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节

可用于存储比 bigint 更大的整型数据
#### 尽可能把所有列定义为 NOT NULL
原因：

- 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间
- 进行比较和计算时要对 NULL 值做特别的处理
## 缓存
执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用。

缓存建立之后，MySQL的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。

缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。 因此，开启缓存查询要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十MB比较合适。此外，还可以通过sql_cache和sql_no_cache来控制某个查询语句是否需要缓存：


```
select sql_no_cache count(*) from usr;
```

## 三种行锁

`InnoDB` 通过 `MVCC` 和 `NEXT-KEY Locks`，解决了在`可重复读`的事务隔离级别下出现`幻读`的问题。`MVCC` 我先挖个坑，日后再细讲，这篇文章我们主要来谈谈那些可爱的锁。

### 什么是幻读？

幻读是在`可重复读`的事务隔离级别下会出现的一种问题，简单来说，`可重复读`保证了当前事务不会读取到其他事务已提交的 `UPDATE`  操作。但同时，也会导致当前事务无法感知到来自其他事务中的 `INSERT` 或 `DELETE` 操作，这就是`幻读`。

### 关于行锁我们要知道的

行锁在 InnoDB 中是基于`索引`实现的，所以一旦某个加锁操作没有使用索引，那么该锁就会退化为`表锁`。



### 记录锁（Record Locks）

顾名思义，记录锁就是为**某行**记录加锁，它`封锁该行的索引记录`：

```
-- id 列为主键列或唯一索引列
SELECT * FROM table WHERE id = 1 FOR UPDATE;
复制代码
```

id 为 1 的记录行会被锁住。

需要注意的是：`id` 列必须为`唯一索引列`或`主键列`，否则上述语句加的锁就会变成`临键锁`。

同时查询语句必须为`精准匹配`（`=`），不能为 `>`、`<`、`like`等，否则也会退化成`临键锁`（感谢评论区 @decodes 提醒）。

#### 其他实现

在通过 `主键索引` 与 `唯一索引` 对数据行进行 UPDATE 操作时，也会对该行数据加`记录锁`：

```
-- id 列为主键列或唯一索引列
UPDATE SET age = 50 WHERE id = 1;
复制代码
```

### 间隙锁（Gap Locks）

**间隙锁**基于`非唯一索引`，它`锁定一段范围内的索引记录`。**间隙锁**基于下面将会提到的`Next-Key Locking` 算法，请务必牢记：**使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据**。

```
SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE;
复制代码
```

即所有在`（1，10）`区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。

除了手动加锁外，在执行完某些 SQL 后，InnoDB 也会自动加**间隙锁**，这个我们在下面会提到。

### 临键锁（Next-Key Locks）

Next-Key 可以理解为一种特殊的**间隙锁**，也可以理解为一种特殊的**算法**。通过**临建锁**可以解决`幻读`的问题。 每个数据行上的`非唯一索引列`上都会存在一把**临键锁**，当某个事务持有该数据行的**临键锁**时，会锁住一段**左开右闭区间**的数据。需要强调的一点是，`InnoDB` 中`行级锁`是基于索引实现的，**临键锁**只与`非唯一索引列`有关，在`唯一索引列`（包括`主键列`）上不存在**临键锁**。

假设有如下表：
 **MySql**，**InnoDB**，**Repeatable-Read**：table(id PK, age KEY, name)

|  id  | age  |  name  |
| :--: | :--: | :----: |
|  1   |  10  |  Lee   |
|  3   |  24  | Soraka |
|  5   |  32  |  Zed   |
|  7   |  45  | Talon  |

该表中 `age` 列潜在的`临键锁`有：
 (-∞, 10],
 (10, 24],
 (24, 32],
 (32, 45],
 (45, +∞],

在`事务 A` 中执行如下命令：

```
-- 根据非唯一索引列 UPDATE 某条记录
UPDATE table SET name = Vladimir WHERE age = 24;
-- 或根据非唯一索引列 锁住某条记录
SELECT * FROM table WHERE age = 24 FOR UPDATE;
复制代码
```

不管执行了上述 SQL 中的哪一句，之后如果在`事务 B` 中执行以下命令，则该命令会被阻塞：

```
INSERT INTO table VALUES(100, 26, 'Ezreal');
复制代码
```

很明显，`事务 A` 在对 `age` 为 24 的列进行 UPDATE 操作的同时，也获取了 `(24, 32]` 这个区间内的临键锁。

不仅如此，在执行以下 SQL 时，也会陷入阻塞等待：

```
INSERT INTO table VALUES(100, 30, 'Ezreal');
复制代码
```

那最终我们就可以得知，在根据`非唯一索引` 对记录行进行 `UPDATE \ FOR UPDATE \ LOCK IN SHARE MODE` 操作时，InnoDB 会获取该记录行的 `临键锁` ，并同时获取该记录行下一个区间的`间隙锁`。

即`事务 A`在执行了上述的 SQL 后，最终被锁住的记录区间为 `(10, 32)`。

> 总结

1. **InnoDB** 中的`行锁`的实现依赖于`索引`，一旦某个加锁操作没有使用到索引，那么该锁就会退化为`表锁`。
2. **记录锁**存在于包括`主键索引`在内的`唯一索引`中，锁定单条索引记录。
3. **间隙锁**存在于`非唯一索引`中，锁定`开区间`范围内的一段间隔，它是基于**临键锁**实现的。
4. **临键锁**存在于`非唯一索引`中，该类型的每条记录的索引上都存在这种锁，它是一种特殊的**间隙锁**，锁定一段`左开右闭`的索引区间。





## 存储引擎

```
show engines;
//查询引擎的类型

show variables like '%storage_engine%';
//查看默认的存储引擎

show table status like "table_name" ;
//查看表的存储引擎
```



早期问题：如何选择MyISAM和Innodb？
现在不存在这个问题了，Innodb不断完善，从各个方面赶超MyISAM，也是MySQL默认使用的。
- MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。
- Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键保证数据完整性。比如OA自动化办公系统。

|                                                              | MyISAM                                          | Innodb                                   |
| ------------------------------------------------------------ | ----------------------------------------------- | ---------------------------------------- |
| 文件格式                                                     | 数据和索引是分别存储的，数据.MYD，索引.MYI      | 数据和索引是集中存储的，.ibd             |
| 文件能否移动                                                 | 能，一张表就对应.frm、MYD、MYI3个文件           | 否，因为关联的还有data下的其它文件       |
| 记录存储顺序                                                 | 按记录插入顺序保存                              | 按主键大小有序插入                       |
| 空间碎片（删除记录并flush table 表名之后，表文件大小不变）   | 产生。定时整理：使用命令optimize table 表名实现 | 不产生                                   |
| 事务                                                         | 不支持                                          | 支持                                     |
| 外键                                                         | 不支持                                          | 支持                                     |
| 锁支持（锁是避免资源争用的一个机制，MySQL锁对用户几乎是透明的） | 表级锁定                                        | 行级锁定、表级锁定，锁定力度小并发能力高 |

### 锁机制与InnoDB锁算法
MyISAM和InnoDB存储引擎使用的锁：
- MyISAM采用表级锁(table-level locking)。
- InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁

表级锁和行级锁对比：
- 表级锁： MySQL中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。
- 行级锁： MySQL中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。

InnoDB存储引擎的锁的算法有三种：
- Record lock：单个行记录上的锁
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- Next-key lock：record+gap 锁定一个范围，包含记录本身

==而导致行锁变为表锁的情况之一就是==：SQL的更新（update）或者删除（delete）语句中未使用到索引，导致在InnoDB在对数据进行相应操作的时候必须把整个表锁起来进行检索（表锁）。而如果使用了索引的话，InnoDB只会通过索引条件检索数据，而只锁住索引对应的行（行锁）。

相关知识点：
1. innodb对于行的查询使用next-key lock
1. Next-locking keying为了解决Phantom Problem幻读问题
1. 当查询的索引含有唯一属性时，将next-key lock降级为record key
1. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
1. 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1


## 范式
实际使用：三大范式只是一般设计数据库的基本理念，可以建立冗余较小、结构合理的数据库。如果有特殊情况，当然要特殊对待，数据库设计最重要的是看需求和性能，需求>性能>表结构。所以不能一味的去追求范式建立数据库

关系数据库中的关系是要满足一定要求的，满足不同程度要求的为不同范式。

- 第一范式（1NF）：符合1NF的关系中的每个属性都不可再分。是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。
- 第二范式（2NF）：2NF在1NF的基础之上，消除了非主属性对于码的部分函数依赖。
- 第三范式（3NF）：3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。
- BCNF：在第三范式基础上，消除主属性对码的部分和传递函数依赖

详细内容参考：[知乎——解释一下关系数据库的第一第二第三范式？_刘慰](https://www.zhihu.com/question/24696366)

## 数据库索引

索引是一种数据结构 。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。

- 大大减少了服务器需要扫描的数据行数。
- 帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。
- 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）

### 主键索引 唯一性索引
- 主键(primary key) 能够唯一标识表中某一行的属性或属性组。一个表只能有一个主键，但可以有多个候选索引。主键常常与外键构成参照完整性约束，防止出现数据不一致。主键可以保证记录的唯一和主键域非空,数据库管理系统对于主键自动生成唯一索引，所以主键也是一个特殊的索引。

- 外键（foreign key） 是用于建立和加强两个表数据之间的链接的一列或多列。外键约束主要用来维护两个表之间数据的一致性。简言之，表的外键就是另一表的主键，外键将两表联系起来。一般情况下，要删除一张表中的主键必须首先要确保其它表中的没有相同外键（即该表中的主键没有一个外键和它相关联）。

- 索引(index) 是用来快速地寻找那些具有特定值的记录。主要是为了检索的方便，是为了加快访问速度， 按一定的规则创建的，一般起到排序作用。所谓唯一性索引，这种索引和前面的“普通索引”基本相同，但有一个区别：索引列的所有值都只能出现一次，即必须唯一。

### 哈希索引和位图
hash索引，("="" ,"in")等值查询效率高，不能排序,不能进行范围查询,不能利用部分索引键查询.不能避免表扫描.遇到大量Hash相同的时候,性能并不比B+树索引高,不稳定.

bitmap,很少支持,类似B+树,内存快,锁的力度比较大.
### B树和B+树的区别

- 在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。

- B+树的叶子节点有一条链相连，而B+树的叶子节点各自独立。

一个m阶的B+树具有如下几个特征：
1. 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。
2. 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。
3. 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素


![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190617210809.png)

### 使用B树的好处

B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。

### 使用B+树的好处
- 索引查找过程中就要产生磁盘I/O消耗,主要看IO次数，和磁盘存取原理有关。==磁盘读写代价低==,根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入,局部性原理与磁盘预读

- 由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。==有利范围查询==,B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 


![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100725.png)

B+树进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。

插入删除操作会破坏平衡树的平衡性，因此在插入删除操作之后，需要对树进行一个分裂、合并、旋转等操作来维护平衡性。

### 数据库为什么使用B+树而不是B树

- ==B树只适合随机检索，而B+树同时支持随机检索和顺序检索；B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组==。可以指定多个列作为索引列，多个索引列共同组成键。
- ==B+树空间利用率更高==，可减少I/O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素；
- ==B+树的查询效率更加稳定==。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。
- ==B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题==。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。
- 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。
- 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。

### 索引为何不采用红黑树

- 红黑树	增加删除，红黑树会进行频繁的调整，来保证红黑树的性质，浪费时间
- B树也就是B-树	B树，查询性能不稳定，查询结果高度不致，每个结点保存指向真实数据的指针，相比B+树每一层每屋存储的元素更多，显得更高一点。
- B+树	B+树相比较于另外两种树,显得更矮更宽，查询层次更浅

### 索引模块（密集索引和稀疏索引）

- 密集索引文件中的每个搜索吗值都对应一个索引值（密集索引保存不仅仅键值，还有其他信息）
- 稀疏索引文件只为索引码的某些值建立索引项（只保存了键值和地址）

InnoDB若一个主键被定义，那么该主键则作为密集索引，若没有主键被定义，那么唯一非空索引为机密索引，若还不满足，则内部自动生产一个隐藏主键（一定要存在，因为非主键索引存储相关键位和其对应的主键值，包含两次查找 ，数据索引在一起）![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190621093059.png)

MyISAM则采用稀疏索引，两颗树存储内容不一样，一颗存储key一颗存储内容（数据索引分开）。

### 索引类型


一个表中可以存在多个普通索引，多个唯一索引，多个候选索引，一个主索。

- 主键索引: 数据列不允许重复，不允许为NULL.一个表只能有一个主键。
- 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。
- 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。
- 聚集索引(Clustered)：表中各行的物理顺序与键值的逻辑（索引）顺序相同，每个表只能有一个
- 非聚集索引(Non-clustered)：非聚集索引指定表的逻辑顺序。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于249个
- 全文索引：针对较大的数据，生成全文索引很耗时好空间。
- 组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则
```
//失效条件
条件是or,如果还想让or条件生效，给or每个字段加个索引
like查询，以%开发
内部函数
对索引列进行计算
```
InnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为==聚簇索引==。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100745.png)

辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100755.png)

### 创建索引
- 建立基础索引：在where、order by、join字段上建立索引。
- 优化，组合索引：基于业务逻辑
- 如果条件经常性出现在一起，那么可以考虑将多字段索引升级为==复合索引==
- 如果通过增加个别字段的索引，就可以出现==索引覆盖==，那么可以考虑为该字段建立索引
- 查询时，不常用到的索引，应该删除掉
创建表之后建立索引

```
create TABLE user_index(
   id int auto_increment primary key,
   first_name varchar(16),
   last_name VARCHAR(16),
   id_card VARCHAR(18),
   information text
);

-- 更改表结构
alter table user_index
-- 创建一个first_name和last_name的复合索引，并命名为name
add key name (first_name,last_name),
-- 创建一个id_card的唯一索引，默认以字段名作为索引名
add UNIQUE KEY (id_card),
-- 鸡肋，全文索引不支持中文
add FULLTEXT KEY (information);
```



创建表时指定索引

```
CREATE TABLE user_index2 (
   id INT auto_increment PRIMARY KEY,
   first_name VARCHAR (16),
   last_name VARCHAR (16),
   id_card VARCHAR (18),
   information text,
   KEY name (first_name, last_name),
   FULLTEXT KEY (information),
   UNIQUE KEY (id_card)
);
```


删除索引
根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名

```
alter table user_index drop KEY name;
alter table user_index drop KEY id_card;
alter table user_index drop KEY information;
```


删除主键索引：

```
alter table 表名 drop primary key（因为主键只有一个）。

```


这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）：需要取消自增长再行删除：

```
alter table user_index
-- 重新定义字段
MODIFY id int,
drop PRIMARY KEY
```

但通常不会删除主键，因为设计主键一定与业务逻辑无关。
### 索引的使用条件

- 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效；

- 对于中到大型的表，索引就非常有效；

- 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。

### 索引的优缺点

作用：
索引可以大大提高系统检索的性能

索引对如下类型的查询有效

- 全值匹配：全值匹配指的是和索引中所有列进行匹配。
- 匹配最左前缀。
- 匹配列前缀：匹配某一列值的开头部分。
- 匹配范围值（索引列顺序存储的特性）。
- 精确匹配某一列并范围匹配另外一列。
- 只访问索引的查询。

当然，索引也存在一些限制

- 不是按照索引的最左列开始查找，则无法使用索引。
- 不能跳过索引中的列。
- 如果查询中某个列是范围查询，则其右边所有的列都无法使用索引优化查找。

优点：

1. 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性
1. 可以大大加快 数据的检索速度，这也是创建索引的最主要的原因
1. 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义
1. 在使用分组和排序 子句进行数据检索时，同样可以显著减少查询中分组和排序的时间
1. 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能

缺点：

- 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加
- 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大
- 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度



### 创建索引时需要注意什么？

- 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；
- 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；
- 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。


适合创建索引：

1. 在经常需要搜索的列上，可以加快搜索的速度。
1. 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构。
1. 在经常用在连接的列上，这 些列主要是一些外键，可以加快连接的速度。
1. 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的。
1. 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。
1. 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。

不适合创建索引：

- 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。
- 对于那些只有很少数据值（唯一性差）的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。
- 对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。
- 当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。


### 最左匹配原则
最左匹配原则是针对索引的

>举例来说：两个字段（name,age）建立联合索引，如果where age=12这样的话，是没有利用到索引的，
这里我们可以简单的理解为先是对name字段的值排序，然后对age的数据排序，如果直接查age的话，这时就没有利用到索引了，
查询条件where name=‘xxx’ and age=xx 这时的话，就利用到索引了，再来思考下where age=xx and name=’xxx‘ 这个sql会利用索引吗，
按照正常的原则来讲是不会利用到的，但是优化器会进行优化，把位置交换下。这个sql也能利用到索引了


- 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
- =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式
### 索引越多越好吗
1. 数据量小的表就不需要建立索引，建立需要额外开销
2.  数据变更需要维护索引，索引维护成本高
3.  索引需要更多空间
### 索引规范
1. 限制每张表上的索引数量,建议单张表索引不超过 5 个
2. 禁止给表中的每一列都建立单独的索引
3. 每个 Innodb 表必须有个主键
4. 常见索引列建议
    - 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列
    - 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段
    - 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好
    - 多表 join 的关联列
5. 如何选择索引列的顺序
建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。

    - 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）
    - 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）
    - 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）

6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）
    - 重复索引示例：primary key(id)、index(id)、unique index(id)
    - 冗余索引示例：index(a,b,c)、index(a,b)、index(a)
7. 对于频繁的查询优先考虑使用覆盖索引

   - 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。
    - 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。

8. 索引 SET 规范：尽量避免使用外键约束

    - 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引
    - 外键可用于保证数据的参照完整性，但建议在业务端实现
    - 外键会影响父表和子表的写操作从而降低性能

## Explain 分析SQL执行是否用到索引
Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。

比较重要的字段有：

- select_type : 查询类型，有简单查询、联合查询、子查询等
    - ALL: 全表扫描
    - index: 索引全扫描
    - range:索引范围扫描
    - ref:使用非唯一索引扫描
    - eq_ref:使用唯一索引扫描
    - const，system:单表中最多只有一个匹配行
- key : 使用的索引
- rows : 扫描的行数
- Extra：关于MYSQL如何解析查询的额外信息坏的例子是Using temporary和Using filesort，意思MYSQL根本不能使用索引，结果是检索会很慢


我们可以通过explain selelct来分析SQL语句执行前的执行计划：
```
//执行计划explain
CREATE TABLE innodb1 (
   id INT auto_increment PRIMARY KEY,
   first_name VARCHAR (16),
   last_name VARCHAR (16),
   id_card VARCHAR (18),
   information text,
   KEY name (first_name, laast_name),
   FULLTEXT KEY (information),
   UNIQUE KEY (id_card)
);
insert into innodb1 (first_name,last_name,id_card,information) values ('张','三','1001','华山派');//会根据主键索引来检索
```
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/5aa6fde9e891999951d93f543da1521d.jpg)



extra列返回的描述的意义

- Distinct:一旦MYSQL找到了与行相联合匹配的行，就不再搜索了

- Not exists: MYSQL优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了

- Range checked for each Record（index map:#）:没有找到理想的索引，因此对于从前面表中来的每一个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一

- Using filesort: 看到这个的时候，查询就需要优化了。MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行

- Using index: 列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候

- Using temporary 看到这个的时候，查询需要优化了。这里，MYSQL需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上

- Where used 使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题不同连接类型的解释（按照效率高低的顺序排序）

- system 表只有一行：system表。这是const连接类型的特殊情况

- const:表中的一个记录的最大值能够匹配这个查询（索引可以是主键或惟一索引）。因为只有一行，这个值实际就是常数，因为MYSQL先读这个值然后把它当做常数来对待

- eq_ref:在连接中，MYSQL在查询时，从前面的表中，对每一个记录的联合都从表中读取一个记录，它在查询使用了索引为主键或惟一键的全部时使用

- ref:这个连接类型只有在查询使用了不是惟一或主键的键或者是这些类型的部分（比如，利用最左边前缀）时发生。对于之前的表的每一个行联合，全部记录都将从表中读出。这个类型严重依赖于根据索引匹配的记录多少—越少越好

- range:这个连接类型使用索引返回一个范围中的行，比如使用>或<查找东西时发生的情况

- index: 这个连接类型对前面的表中的每一个记录联合进行完全扫描（比ALL更好，因为索引一般小于表数据）

## 重构查询方式

### 1. 切分大查询

一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。

```sql
DELETE FROM messages WHERE create < DATE_SUB(NOW(), INTERVAL 3 MONTH);
```

```sql
rows_affected = 0
do {
    rows_affected = do_query(
    "DELETE FROM messages WHERE create  < DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")
} while rows_affected > 0
```

### 2. 分解大连接查询

将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：

- 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。
- 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。
- 减少锁竞争；
- 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。
- 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。

```sql
SELECT * FROM tab
JOIN tag_post ON tag_post.tag_id=tag.id
JOIN post ON tag_post.post_id=post.id
WHERE tag.tag='mysql';
```

```sql
SELECT * FROM tag WHERE tag='mysql';
SELECT * FROM tag_post WHERE tag_id=1234;
SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904);
``` 
## 语法细节
在满足索引使用的场景下（where/order by/join on或索引覆盖），索引也不一定被使用。



MySQL 索引通常是被用于==提高 WHERE 条件的数据行匹配时的搜索速度==，在索引的使用过程中，存在一些使用细节和注意事项。

#### 不要在列上使用函数和进行运算


```
不要在列上使用函数，这将导致索引失效而进行全表扫描。
select * from news where year(publish_time) < 2017

为了使用索引，防止执行全表扫描，可以进行改造。
select * from news where publish_time < '2017-01-01'

还有一个建议，不要在列上进行运算，这也将导致索引失效而进行全表扫描。
elect * from news where id / 100 = 1

为了使用索引，防止执行全表扫描，可以进行改造。
select * from news where id = 1 * 100
```

#### 尽量避免使用 != 或 not in或 <> 等否定操作符
应该尽量避免在 where 子句中使用 != 或 not in 或 <> 操作符，因为这几个操作符都会==导致索引失效而进行全表扫描。==

#### 尽量避免使用 or / in /not in 来连接条件
应该尽量避免在 where 子句中使用 这些 来连接条件，因为这会导致索引失效而进行全表扫描。

```
select * from news where id = 1 or id = 2
```

```
select id from t where num in(1,2,3)

对于连续的数值，能用 between 就不要用 in 了：
select id from t where num between 1 and 3
```
```
select num from a where num in(select num from b)
很多时候用 exists 代替 in 是一个好的选择：
select num from a where exists(select 1 from b where num=a.num)
```

#### 多个单列索引并不是最佳选择
MySQL 只能使用一个索引，会从多个索引中选择一个限制最为严格的索引，因此，为多个列创建单列索引，并不能提高 MySQL 的查询性能。
>假设，有两个单列索引，分别为 news_year_idx(news_year) 和 news_month_idx(news_month)。现在，有一个场景需要针对资讯的年份和月份进行查询，那么，SQL 语句可以写成：
select * from news where news_year = 2017 and news_month = 1

事实上，MySQL 只能使用一个单列索引。==为了提高性能，可以使用复合索引== news_year_month_idx(news_year, news_month) 保证 news_year 和 news_month 两个列都被索引覆盖。

#### 复合索引的最左前缀原则
复合索引遵守“最左前缀”原则，即在查询条件中使用了复合索引的第一个字段，索引才会被使用。因此，在复合索引中索引列的顺序至关重要。如果不是按照索引的最左列开始查找，则无法使用索引。

>假设，有一个场景只需要针对资讯的月份进行查询，那么，SQL 语句可以写成：
select * from news where news_month = 1

此时，无法使用 news_year_month_idx(news_year, news_month) 索引因为遵守“最左前缀”原则，在查询条件中没有使用复合索引的第一个字段，索引是不会被使用的。

#### 覆盖索引的好处
如果一个索引包含所有需要的查询的字段的值，直接根据索引的查询结果返回数据，而无需读表，能够极大的提高性能。因此，可以定义一个让索引包含的额外的列，即使这个列对于索引而言是无用的。
#### 范围查询对多列查询的影响
查询中的某个列有范围查询，则其右边所有列都无法使用索引优化查找。
>举个例子，假设有一个场景需要查询本周发布的资讯文章，其中的条件是必须是启用状态，且发布时间在这周内。那么，SQL 语句可以写成：
select * from news where publish_time >= '2017-01-02' and publish_time <= '2017-01-08' and enable = 1

这种情况下，因为范围查询对多列查询的影响，将导致 news_publish_idx(publish_time, enable) 索引中 publish_time 右边所有列都无法使用索引优化查找。换句话说，news_publish_idx(publish_time, enable) 索引等价于 news_publish_idx(publish_time) 。

对于这种情况，我的建议：对于范围查询，务必要注意它带来的副作用，==并且尽量少用范围查询==，可以通过曲线救国的方式满足业务场景。

例如，上面案例的需求是查询本周发布的资讯文章，==因此可以创建一个news_weekth 字段用来存储资讯文章的周信息==，使得范围查询变成普通的查询，SQL 可以改写成：

```
select * from news where news_weekth = 1 and enable = 1
```

然而，并不是所有的范围查询都可以进行改造，对于必须使用范围查询但无法改造的情况，我的建议：不必试图用 SQL 来解决所有问题，可以使用其他数据存储技术控制时间轴，例如 Redis 的 SortedSet 有序集合保存时间，或者通过缓存方式缓存查询结果从而提高性能。
#### 索引不会包含有NULL值的列
只要列中包含有 NULL 值都将不会被包含在索引中，==复合索引中只要有一列含有 NULL值，那么这一列对于此复合索引就是无效的==。
因此，在数据库设计时，除非有一个很特别的原因使用 NULL 值，不然尽量不要让字段的默认值为 NULL。
#### 隐式转换的影响
当查询条件左右两侧类型不匹配的时候会发生隐式转换，==隐式转换带来的影响就是可能导致索引失效而进行全表扫描==。下面的案例中，date_str 是字符串，然而匹配的是整数类型，从而发生隐式转换。

```
select * from news where date_str = 201701
```

因此，要谨记隐式转换的危害，时刻注意通过同类型进行比较。
#### like 语句的索引失效问题 %XX%不可以，xx%可以
like 的方式进行查询，==在 like “value%” 可以使用索引，但是对于 like “%value%” 这样的方式，执行全表查询，这在数据量小的表，不存在性能问题，但是对于海量数据，全表扫描是非常可怕的事情。所以，根据业务需求，考虑使用 ElasticSearch 或 Solr 是个不错的方案。==

#### 状态值，不容易使用到索引
如性别、支付状态等状态值字段往往只有极少的几种取值可能，这种字段即使建立索引，也往往利用不上。这是因为，一个状态值可能匹配大量的记录，这种情况MySQL会认为利用索引比全表扫描的效率低，从而弃用索引。索引是随机访问磁盘，而全表扫描是顺序访问磁盘，这就好比有一栋20层楼的写字楼，楼底下的索引牌上写着某个公司对应不相邻的几层楼，你去公司找人，与其按照索引牌的提示去其中一层楼没找到再下来看索引牌再上楼，不如从1楼挨个往上找到顶楼。

#### Update 语句
如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。
#### 大量数据分页JOIN
对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。
#### 避免大量索引
索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。

#### 避免无用数据

- 只返回必要的列：最好不要使用 SELECT * 语句。
- 只返回必要的行：使用 LIMIT 语句来限制返回的数据。
- 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。

## 锁
- MyISAM 是表级锁不可以行级锁
- InnoDNM 默认行级锁 也可以表级索。但是遵循2段锁原则：行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。而且用到索引的时候用行级索，不用索引则表级锁。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190621110852.png)


#### 行锁(Record lock记录数)
1. 记录锁, 仅仅锁住索引记录的一行，在单条索引记录上加锁。
2. record lock锁住的永远是索引，而非记录本身，即使该表上没有任何索引，那么innodb会在后台创建一个隐藏的聚集主键索引，那么锁住的就是这个隐藏的聚集主键索引。
所以说当一条sql没有走任何索引时，那么将会在每一条聚合索引后面加X锁，这个类似于表锁，但原理上和表锁应该是完全不同的。




#### 行锁(Gap lock间隙锁)
1. 区间锁, 仅仅锁住一个索引区间（开区间，不包括双端端点）。
2. 在索引记录之间的间隙中加锁，或者是在某一条索引记录之前或者之后加锁，并不包括该索引记录本身。

>比如在 1、2、3中，间隙锁的可能值有 (∞, 1)，(1, 2)，(2, ∞)，
（3）间隙锁可用于防止幻读，保证索引间的不会被插入数据


- 如果where条件全部命中,那么不会用gap锁
- 如果where条件部分命中或者全部命中,则会加gap锁

#### 行锁：临键锁(Next-Key Locks)
1. record lock + gap lock, 左开右闭区间。
2. 默认情况下，innodb使用next-key locks来锁定记录。select … for update
3. 但当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围。
4. Next-Key 

Lock在不同的场景中会退化:
- unique index精确匹配(=),且记录存在:Record Lock
- unique index精确匹配(=),且记录不存在:Gap Lock
-  unique index精确匹配(< >):Record +Gap 锁上届不锁下届

####  行锁：插入意向锁(Insert Intention Locks)
（1）插入意向锁是一种Gap锁，不是意向锁，在insert操作时产生。
（2）在多事务同时写入不同数据至同一索引间隙的时候，并不需要等待其他事务完成，不会发生锁等待。
（3）假设有一个记录索引包含键值4和7，不同的事务分别插入5和6，每个事务都会产生一个加在4-7之间的插入意向锁，获取在插入行上的排它锁，但是不会被互相锁住，因为数据行并不冲突。

（4）插入意向锁不会阻止任何锁，对于插入的记录会持有一个记录锁。
>例如test表存在若干数据的数据，先开始一个事务A，插入一条n=5的数据；（图中步骤1）
此时如果开始一个事务B，执行查询 select * from test where n > 4 for update，事务B会申请Gap锁（4, ∞)，申请成功后，被事务A的x锁阻塞，直到x锁被释放。（图中步骤2）
可以看到图中步骤3的信息，在等待事务释放X锁

#### 快照读：
简单的select操作，没有lock in share mode或for update，快照读不会加任何的锁，而且由于mysql的一致性非锁定读的机制存在，任何快照读也不会被阻塞。但是如果事务的隔离级别是SERIALIZABLE的话，那么快照读也会被加上共享的next-key锁，本文不对SERIALIZABLE隔离级别做叙述。

#### 当前读：
官方文档的术语叫locking read，也就是insert，update，delete,select..in share mode和select..for update,当前读会在所有扫描到的索引记录上加锁，不管它后面的where条件到底有没有命中对应的行记录。当前读可能会引起死锁。

#### 意向锁：
innodb的意向锁主要用户多粒度的锁并存的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。
说白了意向锁的主要作用是处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁”




## 数据库事务

事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。

### 四大特性(简称ACID)

数据库如果支持事务的操作，那么就具备以下四个特性：

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/69B9968816F34399A1DD07421CE7946C.png)

1. 原子性(Atomicity)
    事务是数据库的逻辑工作单位，事务中包括的诸操作要么全做，要么全不做。
2. 一致性(Consistency)
    事务执行的前后，保证数据库一致性（比如AB之间任意转账，他们之间的金额总和不会变）。一致性与原子性是密切相关的。
3. 隔离性(Isolation)
    一个事务的执行不能被其他事务干扰。
4. 持续性/永久性(Durability)
    一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。

### 事务的隔离性

数据库事务的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

1）读未提交： 
2）读已提交： 
3）可重复读： 
4）串行化：

|                                     | 脏读 | 不可重复读 | 幻读 |
| :---------------------------------- | :--: | :--------: | :--: |
| Read uncommitted                    |  √   |     √      |  √   |
| Read committed--Sql Server , Oracle |  ×   |     √      |  √   |
| Repeatable read--MySQL              |  ×   |            |  √   |
| Serializable                        |  ×   |     ×      |  ×   |

- 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。

- 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。

- 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190621114513.png)
### 隔离级别

#### Read uncommitted 读未提交(事物未提交而撤回)

公司发工资了，领导把5000元打到singo的账号上，但是该事务并未提交，而singo正好去查看账户，发现工资已经到账，是5000元整，非常高兴。可是不幸的是，领导发现发给singo的工资金额不对，是2000元，于是迅速回滚了事务，修改金额后，将事务提交，最后singo实际的工资只有2000元，singo空欢喜一场。

出现上述情况，即我们所说的脏读，两个并发的事务，“事务A：领导给singo发工资”、“事务B：singo查询工资账户”，事务B读取了事务A尚未提交的数据。

当隔离级别设置为Read uncommitted时，就可能出现脏读，如何避免脏读，请看下一个隔离级别。

#### Read committed 读提交（一个事物两次查询值不一样，修改了）
==一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。==

singo拿着工资卡去消费，系统读取到卡里确实有2000元，而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到另一账户，并在singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为何......

出现上述情况，即我们所说的不可重复读，两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”，事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。

当隔离级别设置为Read committed时，避免了脏读，但是可能会造成不可重复读。

大多数数据库的默认级别就是Read committed，比如Sql Server , Oracle。如何解决不可重复读这一问题，请看下一个隔离级别。

#### Repeatable read 重复读（一个事物两次查询不同，是其他事物因为插入操作）
==在事务A多次读取构成中，事务B对数据进行了新增操作，导致事务A多次读取的数据不一致。幻读和不可重复读的区别在于，
    不可重复是针对记录的update操作，只要在记录上加写锁，就可避免；幻读是对记录的insert操作，==
    
当隔离级别设置为Repeatable read时，可以避免不可重复读。当singo拿着工资卡去消费时，一旦系统开始读取工资卡信息（即事务开始），singo的老婆就不可能对该记录进行修改，也就是singo的老婆不能在此时转账。

虽然Repeatable read避免了不可重复读，但还有可能出现幻读。

singo的老婆工作在银行部门，她时常通过银行内部系统查看singo的信用卡消费记录。有一天，她正在查询到singo当月信用卡的总消费金额（select sum(amount) from transaction where month = 本月）为80元，而singo此时正好在外面胡吃海塞后在收银台买单，消费1000元，即新增了一条1000元的消费记录（insert transaction ... ），并提交了事务，随后singo的老婆将singo当月信用卡消费的明细打印到A4纸上，却发现消费总额为1080元，singo的老婆很诧异，以为出现了幻觉，幻读就这样产生了。

注：MySQL的默认隔离级别就是Repeatable read。

#### Serializable 序列化

Serializable是最高的事务隔离级别，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。

### 总结
- 一级封锁协议是：事务T在修改数据R之前必须先对其加X锁，直到事务结束才释放。事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。
- 一级封锁协议可以防止丢失修改，并保证事务T是可恢复的。使用一级封锁协议可以解决丢失修改问题。 它不能保证可重复读和不读“脏”数据。 
- 二级封锁协议是：一级封锁协议加上事务T在读取数据R之前必须先对其加S锁，读完后方可释放S锁。 　
- 二级封锁协议除防止了丢失修改，还可以进一步防止读“脏”数据。但在二级封锁协议中，由于读完数据后即可释放S锁，所以它不能保证可重复读。 　
- 三级封锁协议是：一级封锁协议加上事务T在读取数据R之前必须先对其加S锁，直到事务结束才释放。 
- 三级封锁协议除防止了丢失修改和不读“脏”数据外，还进一步防止了不可重复读。
#### Read Uncommitted（读取未提交内容）

在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。

#### Read Committed（读取提交内容）

这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。

#### Repeatable Read（可重读）

这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。

**InnoDB 和 XtraDB存储引擎 通过多版本并发控制（ MVCC， Multiversion Concurrency Control） 解决了幻读的问题。**

#### Serializable（可串行化）

这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。

InnoDB 存储引擎在 分布式事务 的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。
### 隔离级别与锁的关系

1. 在Read Uncommitted级别下，读操作不加S锁；
1.  在Read Committed级别下，读操作需要加S锁，但是在语句执行完以后释放S锁；
1.  在Repeatable Read级别下，读操作需要加S锁，但是在事务提交之前并不释放S锁，也就是必须等待事务执行完毕以后才释放S锁。
1.  在Serialize级别下，会在Repeatable Read级别的基础上，添加一个范围锁。保证一个事务内的两次查询结果完全一样，而不会出现第一次查询结果是第二次查询结果的子集。


## 存储过程

存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。

### 优点

- 1）存储过程是预编译过的，执行效率高。
-  2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。
-  3）安全性搞，执行存储过程需要有一定权限的用户。
-  4）存储过程可以重复使用，减少数据库开发人员的工作量。

### 缺点

- 1）调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。
-  2）移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。
-  3）重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。
-  4）如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。

## 视图

视图是从一个或几个基本表（或视图）导出的表。它与基本表不同，是一个虚表。数据库中只存放视图的定义，而不存放视图对应的数据，这些数据仍存放在原来的基本表中。所以一旦基本表中的数据发生变化，从视图中查询出的数据也就随之改变了。从这个意义上讲，视图就像一个窗口，透过它可以看到数据库中自己感兴趣的数据及其变化。
 视图一经定义，就可以和基本表一样被查询、被删除。也可以在一个视图上再定义新的视图，但对视图的更新（增、删、改）操作则有一定的限制。

### 视图的优点

1. 查询简单化。视图能简化用户的操作
2. 数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护
3. 逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性

### 视图的缺点

1. 性能。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。
2. 修改限制。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，对于比较复杂的视图，可能是不可修改的，这些视图有如下特征：
    　　a.有UNIQUE等集合操作符的视图。
    　　b.有GROUP BY子句的视图。
    　　c.有诸如AVG\SUM\MAX等聚合函数的视图。
    　　d.使用DISTINCT关键字的视图。
    　　e.连接表的视图（其中有些例外）

## 游标
**游标实际上是一种能从包括多条数据记录的结果集（结果集是select查询之后返回的所有行数据的集合）中每次提取一条记录的机制充当指针的作用，遍历结果中的所有行，但他一次只指向一行**。游标的结果集是由SELECT语句产生，如果处理过程需要重复使用一个记录集，那么创建一次游标而重复使用若干次，比重复查询数据库要快的多。

==当于一个“书签”，当你逐行取数据的时候，它告诉数据库，你上次取到哪里了。==

也可以说，SQL的游标是一种临时的数据库对象，可以用来存放在数据库表中的数据行副本，也可以指向存储在数据库中的数据行的指针。游标提供了在逐行的基础上操作表中数据的方法。

一般复杂的存储过程，都会有游标的出现，他的用处主要有： 
1. 定位到结果集中的某一行。 
2. 对当前位置的数据进行读写。 
3. 可以对结果集中的数据单独操作，而不是整行执行相同的操作。 
4. 是面向集合的数据库管理系统和面向行的程序设计之间的桥梁。

不足： 
==数据量小时才使用游标==，因为： 
1. 游标使用时会对行加锁，系统上跑的不只我们一个业务，这就会影响其他业务的正常进行； 
2. 数据量大时其效率也较低效； 
3. 游标其实是相当于把磁盘数据整体放入了内存中，如果游标数据量大则会造成内存不足， 


游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果，每个游标区都有一个名字。用户可以通过游标逐一获取记录并赋给主变量，交由主语言进一步处理。

在存储过程中使用游标可以对一个结果集进行移动遍历。

游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。

使用游标的四个步骤：

1. 声明游标，这个过程没有实际检索出数据；
2. 打开游标；
3. 取出数据；
4. 关闭游标；

```sql
delimiter //
create procedure myprocedure(out ret int)
    begin
        declare done boolean default 0;

        declare mycursor cursor for
        select col1 from mytable;
        # 定义了一个 continue handler，当 sqlstate '02000' 这个条件出现时，会执行 set done = 1
        declare continue handler for sqlstate '02000' set done = 1;

        open mycursor;

        repeat
            fetch mycursor into ret;
            select ret;
        until done end repeat;

        close mycursor;
    end //
 delimiter ;
```
## 触发器

触发器是特殊类型的存储过程，它由事件触发，而不是程序调用或者手工调用，自动完成这些sql语句。使用触发器可以保证数据的完整性和有效性。他分事先触发和事后触发。
- 事先：验证一些条件或进行一些准备工作
- 事后：保证事务完整性
- 触发事件分为三类：==UPDATE、DELETE和INSERT==。

一旦定义，触发器将被保存在数据库服务器中。任何用户对表的增、删、改操作均由服务器自动激活相应的触发器，在关系数据库管理系统核心层进行集中的完整性控制。触发器类似于约束，但是比约束更加灵活，可以实施更为复杂的检查和操作，具有更精细和更强大的数据控制能力。

触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。

触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。

INSERT 触发器包含一个名为 NEW 的虚拟表。

```sql
CREATE TRIGGER mytrigger AFTER INSERT ON mytable
FOR EACH ROW SELECT NEW.col into @result;

SELECT @result; -- 获取结果
```

DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。

UPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改的，而 OLD 是只读的。

- MySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。
- 触发器不能有Commit


```
//作用
1.安全性。可以基于数据库的值使用户具有操作数据库的某种权利。

  # 可以基于时间限制用户的操作，例如不允许下班后和节假日修改数据库数据。

  # 可以基于数据库中的数据限制用户的操作，例如不允许股票的价格的升幅一次超过10%。

2.审计。可以跟踪用户对数据库的操作。   

  # 审计用户操作数据库的语句。

  # 把用户对数据库的更新写入审计表。

3.实现复杂的数据完整性规则

  # 实现非标准的数据完整性检查和约束。触发器可产生比规则更为复杂的限制。与规则不同，触发器可以引用列或数据库对象。例如，触发器可回退任何企图吃进超过自己保证金的期货。


  # 提供可变的缺省值。


4.实现复杂的非标准的数据库相关完整性规则。触发器可以对数据库中相关的表进行连环更新。例如，在auths表author_code列上的删除触发器可导致相应删除在其它表中的与之匹配的行。

  # 在修改或删除时级联修改或删除其它表中的与之匹配的行。

  # 在修改或删除时把其它表中的与之匹配的行设成NULL值。

  # 在修改或删除时把其它表中的与之匹配的行级联设成缺省值。

  # 触发器能够拒绝或回退那些破坏相关完整性的变化，取消试图进行数据更新的事务。当插入一个与其主健不匹配的外部键时，这种触发器会起作用。例如，可以在books.author_code 列上生成一个插入触发器，如果新值与auths.author_code列中的某值不匹配时，插入被回退。

5.同步实时地复制表中的数据。


6.自动计算数据值，如果数据的值达到了一定的要求，则进行特定的处理。例如，如果公司的帐号上的资金低于5万元则立即给财务人员发送警告数据。
```


## 分区 
数据量大的适合采用分区，需要将数据分到多个存储文件中，保证单个存储文件的处理效率。

一般情况下我们创建的表对应一组存储文件，使用MyISAM存储引擎时是一个.MYI和.MYD文件，使用Innodb存储引擎时是一个.ibd和.frm（表结构）文件。
当数据量较大时（一般千万条记录级别以上），MySQL的性能就会开始下降，这时我们就需要将数据分散到多组存储文件，==保证其单个文件的执行效率==。

最常见的分区方案是按id分区，如下将id的哈希值对10取模将数据均匀分散到10个.ibd存储文件中：

```
create table article(
   id int auto_increment PRIMARY KEY,
   title varchar(64),
   content text
)PARTITION by HASH(id) PARTITIONS 10
```
### MySQL提供的分区算法

- hash(field)：
相同的输入得到相同的输出。输出的结果跟输入是否具有规律无关。==仅适用于整型字段==

- key(field)：
和hash(field)的性质一样，只不过key是==处理字符串==的，比hash()多了一步从字符串中计算出一个整型在做取模操作。

```
create table article_key(
   id int auto_increment,
   title varchar(64),
   content text,
   PRIMARY KEY (id,title)  -- 要求分区依据字段必须是主键的一部分
)PARTITION by KEY(title) PARTITIONS 10
```

- range算法：
是一种==条件分区==算法，按照数据大小范围分区（将数据使用某种条件，分散到不同的分区中）。

    如下，按文章的发布时间将数据按照2018年8月、9月、10月分区存放：
    
    注意：条件运算符只能使用==less than==，这以为着较小的范围要放在前面，比如上述p201808,p201819,p201810分区的定义顺序依照created_time数值范围从小到大，不能颠倒
```
create table article_range(
   id int auto_increment,
   title varchar(64),
   content text,
   created_time int,   -- 发布时间到1970-1-1的毫秒数
   PRIMARY KEY (id,created_time)   -- 要求分区依据字段必须是主键的一部分
)charset=utf8
PARTITION BY RANGE(created_time)(
   PARTITION p201808 VALUES less than (1535731199),    -- select UNIX_TIMESTAMP('2018-8-31 23:59:59')
   PARTITION p201809 VALUES less than (1538323199),    -- 2018-9-30 23:59:59
   PARTITION p201810 VALUES less than (1541001599) -- 2018-10-31 23:59:59
);
```
- list算法

也是一种条件分区，按照列表值分区（in (值列表)）。

```
create table article_list(
   id int auto_increment,
   title varchar(64),
   content text,
   status TINYINT(1),  -- 文章状态：0-草稿，1-完成但未发布，2-已发布
   PRIMARY KEY (id,status) -- 要求分区依据字段必须是主键的一部分
)charset=utf8
PARTITION BY list(status)(
   PARTITION writing values in(0,1),   -- 未发布的放在一个分区  
   PARTITION published values in (2)   -- 已发布的放在一个分区
);
insert into article_list values(null,'mysql优化','内容示例',0);
flush tables;
```
### 分区管理语法
- range/list

```
增加分区
前文中我们尝试使用range对文章按照月份归档
随着时间的增加，我们需要增加一个月份：
alter table article_range add partition(
   partition p201811 values less than (1543593599) -- select UNIX_TIMESTAMP('2018-11-30 23:59:59')
   -- more
);
```


```
删除分区
alter table article_range drop PARTITION p201808
注意：删除分区后，分区中原有的数据也会随之删除！
```

```
key/hash
新增分区
alter table article_key add partition partitions 4
```

```
销毁分区
alter table article_key coalesce partition 6
key/hash分区的管理不会删除数据，但是每一次调整（新增或销毁分区）都会将所有的数据重写分配到新的分区上。==效率极低==，最好在设计阶段就考虑好分区策略。
```
分区的使用
当数据表中的数据量很大时，分区带来的效率提升才会显现出来。
只有检索字段为分区字段时，分区带来的效率提升才会比较明显。因此，==分区字段的选择很重要==，并且==业务逻辑要尽可能地根据分区字段做相应调整==（尽量使用分区字段作为查询条件）。
## 水平分割和垂直分割
分表原因
- 为数据库减压
- 分区算法局限
- 数据库支持不完善（5.1之后mysql才支持分区操作）

id重复的解决方案
- 借用第三方应用如memcache、redis的id自增器
- 单独建一张只包含id一个字段的表，每次自增该字段作为数据记录的id

当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。


![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100810.png)
垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100825.png)

## Sharding
###  策略

- 哈希取模：hash(key) % N；
- 范围：可以是 ID 范围也可以是时间范围；
- 映射表：使用单独的一个数据库来存储映射关系。

###  存在的问题
 1. 事务问题

使用分布式事务来解决，比如 XA 接口。

2. 连接

可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。

 3. ID 唯一性

- 使用全局唯一 ID（GUID）
- 为每个分片指定一个 ID 范围
- 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)
## 集群
横向扩展：从根本上（单机的硬件处理能力有限）提升数据库性能 。由此而生的相关技术：==读写分离、负载均衡==
### 主从复制
将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。

主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。

-  **binlog 线程** ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。
-  **I/O 线程** ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。
-  **SQL 线程** ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100834.png)

#### 主从复制的作用

1. 主数据库出现问题，可以切换到从数据库。
2. 可以进行数据库层面的读写分离。
3. 可以在从数据库上进行日常备份。

#### 复制过程



![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190617210831.png)

Binary log：主数据库的二进制日志
Relay log：从服务器的中继日志
-  第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。
-  第二步：salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。
-  第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。
### 读写分离
读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。

主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。

读写分离能提高性能的原因在于：

- 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
- 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
- 增加冗余，提高可用性。

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611100844.png)


### 负载均衡
负载均衡算法
- 轮询
- 加权轮询：按照处理能力来加权
- 负载分配：依据当前的空闲状态（但是测试每个节点的内存使用率、CPU利用率等，再做比较选出最闲的那个，效率太低）
### 高可用
在服务器架构时，为了保证服务器7x24不宕机在线状态，需要为每台单点服务器（由一台服务器提供服务的服务器，如写服务器、数据库中间件）提供冗余机。

对于写服务器来说，需要提供一台同样的写-冗余服务器，当写服务器健康时（写-冗余通过心跳检测），写-冗余作为一个从机的角色复制写服务器的内容与其做一个同步；当写服务器宕机时，写-冗余服务器便顶上来作为写服务器继续提供服务。对外界来说这个处理过程是透明的，即外界仅通过一个IP访问服务。
## 慢查询日志

一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中，定位查询效率的SQL，针对进行优化。





## 字段设计
整数：
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190617132345.png)
字符串：
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190617132447.png)
时间和日期：
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190617132507.png)
复合类型：

- ENUM 类型因为只允许在集合中取得一个值或使用null值，有点类似于单选项。（比如人的性别）ENUM 类型字段可以从集合中取得一个值或使用 null 值，除此之外的输入将会使 MySQL 在这个字段中插入一个空字符串。另外如果插入值的大小写与集合中值的大小写不匹配，MySQL 会自动使用插入值的大小写转换成与集合中大小写一致的值。

- SET 类型可以从预定义的集合中取得任意数量的值。并且与 ENUM 类型相同的是任何试图在 SET 类型字段中插入非预定义的值都会使 
MySQL 插入一个空字符串。如果插入一个即有合法的元素又有非法的元素的记录，MySQL 将会保留合法的元素，除去非法的元素。

CHAR和VARCHAR：
- ==CHAR为定长==，必须在圆括号内用一个大小修饰符来定义,比指定长度大的值将被截短，而比指定长度小的值将会用空格作填补；==VARCHAR是一种可变长度的字符串类型==，并且也必须带有一个范围在 0-255 之间的指示器。
- CHAR 把这个大小视为值的大小，不长度不足的情况下就用空格补足。而 VARCHAR 类型把它视为最大值并且只使用存储字符串实际需要的长度(增加一个额外字节来存储字符串本身的长度)来存储值。所以短于指示器长度的 VARCHAR 类型不会被空格填补，但长于指示器的值仍然会被截短。
- 都可以使用BINARY修饰符。用于比较运算式，以二进制方式参与运算。
- 定长速度快，定长表存储同样内容数据，占用的磁盘空间更大。

## 优化思路

```
//查看慢查询
show variables like %quer%
```



1. 查看慢查询，开启慢查询 设置1秒以上显示，超过1秒就已经慢了。

```
set global slow_query_log = on;
set global show_query_log_fiter = 1;
```
找到日志地址，打开查看

2. 用explain查看
其中主要看两列：type（性能查，全表扫描）和extra（USing filesort外排序和using temporary临时表，这两个比较慢需要优化）
3. 修改SQL或者走索引，

## 大表数据查询，怎么优化
对于大表使用 pt-online-schema-change 修改表结构
- 避免大表修改产生的主从延迟
- 避免在对表字段进行修改时进行锁表

对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。

pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。
### 为什么要优化
- 系统的吞吐量瓶颈往往出现在数据库的访问速度上
- 随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢
- 数据是存放在磁盘上的，读写速度无法和内存相比

### 优化方向
查询慢是看慢查询日志，没有用索引就加上，用了索引就看是数据量太大还是表字段太长，最后不行就丢es里。
1. 设计数据库时：字段的设计过长，存储引擎的选择错误
1. 慢查询
1. 没有索引或者没有用到索引(这是查询慢最常见的问题，是程序设计的缺陷)
1. I/O吞吐量小，形成了瓶颈效应。数据是存放在磁盘上的，读写速度无法和内存相比
1. 没有创建计算列导致查询不优化。
1. 内存不足
1. 网络速度慢，访问结构传输小
1. 查询出的数据量过大（可以采用多次查询或其他的方法降低数据量）
1. 锁或者死锁(这也是查询慢最常见的问题，是程序设计的缺陷)sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。
1. 返回了不必要的行和列
1. 查询语句不好，没有优化，SQL语句的优化（收效甚微）
2. 横向扩展：MySQL集群、负载均衡、读写分离

可以通过如下方法来优化查询 :
1. 把数据、日志、索引放到不同的I/O设备上，增加读取速度，以前可以将Tempdb应放在RAID0上，SQL2000不在支持。数据量（尺寸）越大，提 高I/O越重要.
2. 纵向、横向分割表，减少表的尺寸(sp_spaceuse)
3. 升级硬件
4. 根据查询条件,建立索引,优化 索引、优化访问方式，限制结果集的数据量。注意填充因子要适当（最好是使用默认值0）。索引应该尽量小，使用字节数小的列建索引好（参照索引的创建）,不 要对有限的几个值的字段建单一索引如性别字段
5. 提高网速;
6. 扩大服务器的内存
7. 增加服务器CPU个数;但是必须明白并行处理串行处理更需要资源例如内存。使用并行还是串行程是MsSQL自动评估选择的。单个任务分解成多个任务，就可以在 处理器上运行。例如耽搁查询的排序、连接、扫描和GROUP BY字句同时执行，SQL SERVER根据系统的负载情况决定最优的并行等级，复杂的需要消耗大量的CPU的查询最适合并行处理。但是更新操作 UPDATE,INSERT，DELETE还不能并行处理。
8. 如果是使用like进行查询的话，简单的使用index是不行的，全文索引耗空间。






>问题分析：从1000万这个数字可以定位到考察数据量特别大的情况下如何提升查询效率。
对sql语句优化

- 子查询变成left join
- limit 分布优化，先利用ID定位，再分页
- or条件优化，多个or条件可以用union all对结果进行合并（union all结果可能重复）
- 不必要的排序
- where代替having,having 检索完所有记录，才进行过滤
- 避免嵌套查询
- 对多个字段进行等值查询时，联合索引 

数据库设计方面

- 建立索引
- 数据表分区
- 尽量使用固定长度的字段
- 限制字段长度
- 分表，分库

数据库I/O方面

1. 增加缓冲器
1. 涉及表的级联，不同的表存储在不同的磁盘上，以增加I/O速度
1. 改善物理设备，比如更换固态硬盘

在SQL语句方面

1. 优化SQL语句，较少比较次数
1. 限制返回的记录数，进行分页查询
1. 在Java方面
1. 反复查询，使用preparedStatement减少查询次数
1. 考虑数据缓存，较少数据库访问

超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作
大批量操作可能会造成严重的主从延迟

主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间， 而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况

binlog 日志为 row 格式时会产生大量的日志

大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因

避免产生大事务操作

大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。

特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批

### 优化举例
下面通过几个真实场景的例子来看看如何进行优化。


```
select
   distinct cert.emp_id 
from
   cm_log cl 
inner join
   (
      select
         emp.id as emp_id,
         emp_cert.id as cert_id 
      from
         employee emp 
      left join
         emp_certificate emp_cert 
            on emp.id = emp_cert.emp_id 
      where
         emp.is_deleted=0
   ) cert 
      on (
         cl.ref_table='Employee' 
         and cl.ref_oid= cert.emp_id
      ) 
      or (
         cl.ref_table='EmpCertificate' 
         and cl.ref_oid= cert.cert_id
      ) 
where
   cl.last_upd_date >='2013-11-07 15:03:00' 
   and cl.last_upd_date<='2013-11-08 16:00:00';
```

先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢。下面是EXPLAIN的结果


```
+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+
| id | select_type | table      | type  | possible_keys                   | key                   | key_len | ref               | rows  | Extra                          |
+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+
|  1 | PRIMARY     | cl         | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date     | 8       | NULL              |   379 | Using where; Using temporary   |
|  1 | PRIMARY     | &lt;derived2&gt; | ALL   | NULL                            | NULL                  | NULL    | NULL              | 63727 | Using where; Using join buffer |
|  2 | DERIVED     | emp        | ALL   | NULL                            | NULL                  | NULL    | NULL              | 13317 | Using where                    |
|  2 | DERIVED     | emp_cert   | ref   | emp_certificate_empid           | emp_certificate_empid | 4       | meituanorg.emp.id |     1 | Using index                    |
+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+
```

简述一下执行计划

首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；

然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。

我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows = 1表示，每个关联都只锁定了一条记录，效率比较高。

获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。

如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？

仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来。

注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union all了，因为使用union需要去重的动作，会影响SQL性能。

优化过的语句如下


```
select
   emp.id 
from
   cm_log cl 
inner join
   employee emp 
      on cl.ref_table = 'Employee' 
      and cl.ref_oid = emp.id  
where
   cl.last_upd_date >='2013-11-07 15:03:00' 
   and cl.last_upd_date<='2013-11-08 16:00:00' 
   and emp.is_deleted = 0  
union
select
   emp.id 
from
   cm_log cl 
inner join
   emp_certificate ec 
      on cl.ref_table = 'EmpCertificate' 
      and cl.ref_oid = ec.id  
inner join
   employee emp 
      on emp.id = ec.emp_id  
where
   cl.last_upd_date >='2013-11-07 15:03:00' 
   and cl.last_upd_date<='2013-11-08 16:00:00' 
   and emp.is_deleted = 0
```

用改造后的语句实验一下，只需要10ms 降低了近200倍！


```
+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+
| id | select_type  | table      | type   | possible_keys                   | key               | key_len | ref                   | rows | Extra       |
+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+
|  1 | PRIMARY      | cl         | range  | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8       | NULL                  |  379 | Using where |
|  1 | PRIMARY      | emp        | eq_ref | PRIMARY                         | PRIMARY           | 4       | meituanorg.cl.ref_oid |    1 | Using where |
|  2 | UNION        | cl         | range  | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8       | NULL                  |  379 | Using where |
|  2 | UNION        | ec         | eq_ref | PRIMARY,emp_certificate_empid   | PRIMARY           | 4       | meituanorg.cl.ref_oid |    1 |             |
|  2 | UNION        | emp        | eq_ref | PRIMARY                         | PRIMARY           | 4       | meituanorg.ec.emp_id  |    1 | Using where |
| NULL | UNION RESULT | &lt;union1,2&gt; | ALL    | NULL                            | NULL              | NULL    | NULL                  | NULL |             |
+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+---
```


## 更多优化
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/693fd75fcc986d0749f430a44d986e8d.jpg)