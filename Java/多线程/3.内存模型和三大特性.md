[toc]

# JAVA内存模型
JMM是一个抽象的概念,并不真实存在,他描述的是一组规范,通过这组规范定义了程序中各个变量的访问方式.

通过这些规则来规范对内存的读写操作，从而保证指令执行的正确性。它与处理器有关、与缓存有关、与并发有关、与编译器也有关。

内存模型解决并发问题主要采用两种方式：

- 限制处理器优化
- 使用内存屏障

简要言之，jmm是jvm的一种规范，定义了jvm的内存模型。它屏蔽了各种硬件和操作系统的访问差异，不像c那样直接访问硬件内存，相对安全很多，它的主要目的是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题。可以保证并发编程场景中的原子性、可见性和有序性。



## 线程通信可见性
Java 内存模型（Java Memory Model，JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了 Java 程序在各种平台下对内存的访问都能保证效果一致的==机制及规范==。

JMM主内存:
1. 存储Java实例对象
2. 成员变量/类信息/常量/静态变量等
3. 属于数据共享的区域,多线程并发操作时会引发线程安全问题.

在 Java 中，所有实例域、静态域 和 数组元素存储在堆内存中，堆内存在线程之间共享。局部变量、方法定义参数 和 异常处理器参数 不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。


JMM工作内存:
1. 当前方法的所有本地变量信息,本地变量堆其他线程不可见
2. 字节码行号指示器,Native方法信息
3. 属于线程私有数据区域,不存在线程安全问题

Java 线程之间的通信由 Java 内存模型（JMM）控制。JMM 决定了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程与主内存之间的抽象关系：==线程之间的共享变量存储在主内存中，每一个线程都有一个自己私有的本地内存，本地内存中存储了该变量以读／写共享变量的副本==。本地内存是 JMM 的一个抽象概念，并不真实存在。


==每一个线程都有一个自己私有的本地内存，本地内存中存储了该变量以读／写共享变量的副本.==  线程之间的通信机制：共享内存和消息传递。线程之间的通信都是隐式的。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsUntitled%20Diagram.png)


>举个例子：本地内存 A 和 B 有主内存共享变量 X 的副本。假设一开始时，这三个内存中 X 的值都是 0。线程 A 正执行时，把更新后的 X 值（假设为 1）临时存放在自己的本地内存 A 中。当线程 A 和 B 需要通信时，线程 A 首先会把自己本地内存 A 中修改后的 X 值刷新到主内存去，此时主内存中的 X 值变为了 1。随后，线程 B 到主内存中读取线程 A 更新后的共享变量 X 的值，此时线程 B 的本地内存的 X 值也变成了 1。
整体来看，这两个步骤实质上是线程 A 再向线程 B 发送消息，而这个通信过程必须经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 Java 程序员提供内存可见性保证。

>哪些是共享变量

在java程序中所有实例域，静态域和数组元素都是放在堆内存中（所有线程均可访问到，是可以共享的），而局部变量，方法定义参数和异常处理器参数不会在线程间共享。共享数据会出现线程安全的问题，而非共享数据不会出现线程安全的问题。


==JMM和Java内存区域是不同概念:==
- JMM描述的是规则,围绕三大特性
- 相似点:都是有共享和私有区域
- 方法内的基本数据类型存储内存栈帧
- 方法内引用类型:引用存放工作内存,实例放在主内存
- 成员变量,static变量,类信息加载到主内存
- 主内存共享方式是线程各拷贝一份数据到工作你村,操作完成后刷新回主内存


### 内存屏障
CPU中，每个CPU又有多级缓存，一般分为L1,L2,L3，因为这些缓存的出现，提高了数据访问性能，避免每次都向内存索取，但是弊端也很明显，不能实时的和内存发生信息交换，分在不同CPU执行的不同线程对同一个变量的缓存值不同。


>为什么需要内存屏障?

由于现代操作系统都是多处理器操作系统，每个处理器都会有自己的缓存，可能存再不同处理器缓存不一致的问题，而且由于操作系统可能存在重排序，导致读取到错误的数据，因此，操作系统提供了一些内存屏障以解决这种问题.
简单来说:
1. 在不同CPU执行的不同线程对同一个变量的缓存值不同，为了解决这个问题。
2. 用volatile可以解决上面的问题，不同硬件对内存屏障的实现方式不一样。java屏蔽掉这些差异，通过jvm生成内存屏障的指令。
对于读屏障:在指令前插入读屏障，可以让高速缓存中的数据失效，强制从主内存取。

>内存屏障的作用

cpu执行指令可能是无序的，它有两个比较重要的作用
1. *阻止屏障两侧指令重排序*
2. *强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效*。


Java线程同步机制就是使用内存屏障在具体实现的。JMM提供的可见性规，而产生这样的原理就是“内存屏障” :==在指令序列中就像一堵墙一样使其两侧的指令无法穿越（即不可以重排序）==

JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略：
- 在每个volatile写操作的前面插入一个StoreStore屏障。
- 在每个volatile写操作的后面插入一个StoreLoad屏障。
- 在每个volatile读操作的后面插入一个LoadLoad屏障。
- 在每个volatile读操作的后面插入一个LoadStore屏障。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs06789ad0b8108f59398b34c10a8cd352.jpg)

```
class VolatileBarrierExample {
    int a;
    volatile int v1 = 1;
    volatile int v2 = 2;

    void readAndWrite() {
        int i = v1;           //第一个volatile读
        int j = v2;           // 第二个volatile读
        a = i + j;            //普通写
        v1 = i + 1;          // 第一个volatile写
        v2 = j * 2;          //第二个 volatile写
    }

    …                    //其他方法
}
```
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsimage.png)

内存屏障在锁中的使用：
- 获取锁
- 加载屏障
- 获取屏障
- 临界区
- 释放屏障
- 存储屏障
- 其中：3和5用来禁止指令重排序





### 内存
JMM模型解答了线程安全方面的问题。它规定：
1. 对于long/double型以外的数据类型以及引用类型的共享变量进行读、写操作都具有原子性。对于volatile修饰的long/double型变量读、写操作也具有原子性
2. 可见性和有序性则用happen-before原则来解释。

JMM决定一个线程对共享变量的写入何时对另一个线程可见：

1. 每个线程都拥有自己的线程栈。
1. 线程栈包含了当前线程执行的方法调用相关信息，我们也把它称作调用栈。
1. 线程栈还包含了当前方法的所有本地变量信息。
1. 一个线程只能读取自己的线程栈，每个线程中的本地变量都会有自己的版本，线程中的本地变量对其它线程是不可见的。
1. 所有原始类型(8种)的本地变量都直接保存在线程栈当中，线程可以传递原始类型的副本给另一个线程，线程之间无法共享原始类型的本地变量。
1. 堆区包含了Java应用创建的所有对象信息。
1. 堆区包含了所有线程创建的对象信息。
1. 堆区包含了原始类型的封装类（如Byte、Integer、Long等等）的对象信息。

调用栈和本地变量都存储在栈区，对象都存储在堆区

1. 局部的基本类型：完全存储于Thread Stack中。
1. 局部的对象引用：引用会被存储于Thread Stack中，对象本身会被存储于Heap中。
1. 对象的成员方法中的局部变量：如果方法中包含本地变量，则会存储在Thread Stack中。
1. 对象的成员变量：不管它是原始类型还是包装类型，都会被存储到Heap区。
1. Static类型的变量：存储于Heap中。
1. 类本身相关信息：存储于Heap中。

### 主内存与工作内存
处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。

加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611103933.png)
所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。

线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611104453.png)

### 内存间交互操作
Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。


![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs20190611104530.png)

```
read：把一个变量的值从主内存传输到工作内存中
load：在 read 之后执行，把 read 得到的值放入工作内存的变量副本中
use：把工作内存中一个变量的值传递给执行引擎
assign：把一个从执行引擎接收到的值赋给工作内存的变量
store：把工作内存的一个变量的值传送到主内存中
write：在 store 之后执行，把 store 得到的值放入主内存的变量中
lock：作用于主内存的变量
unlock :解锁
```
假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0。具体的原因如下图所示：
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsf7ba67f413f65876242fdf1fdc06df6f.jpg)

处理器 A 和 B 同时把共享变量写入在写缓冲区中（A1、B1），然后再从内存中读取另一个共享变量（A2、B2），最后才把自己写缓冲区中保存的脏数据刷新到内存中（A3、B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。

从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1 -> A2，但内存操作实际发生的顺序却是：A2 -> A1。此时，处理器 A 的内存操作顺序被重排序了。

这里的关键是，==由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致==。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。

## 禁止指令的重排序
### 重排序

==无法通过Happen-Before原则推导出来的才能进行指令的重排序.==

在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三类：
1. 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
2. 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
3. 内存系统的重排序。由于处理器使用缓存和读／写缓冲区，这使得加载和存储操作看


JMM在具体实现上的基本方针: ==在不改变（正确同步的）程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门==。
但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子：

```
int a = 10;    //语句1
int r = 2;    //语句2
a = a + 3;    //语句3
r = a*a;     //语句4
```

 　　这段代码有4个语句，那么可能的执行顺序是：  
```
2 - 1 - 3 - 4  or
1 - 2 - 3 - 4
```

### 数据依赖
因为处理器在进行重排序时是会考虑指令之间的数据依赖性。
　　那么可不可能是这个执行顺序呢： 
```
语句2   语句1    语句4   语句3
```
不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。

如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型：

```
写后读 a = 1; b = a; 写一个变量之后，再读这个位置。
写后写 a = 1; a = 2; 写一个变量之后，再写这个变量。
读后写 a = b; b = 1; 读一个变量之后，再写这个变量。
```

上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。

前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。

注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。

###  final 可以影响什么

final 域的重排序规则
- 1）在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给以额引用变量，这两个操作之间不能重排序
- 2）初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序
- 3）final 域为引用类型时，增加了如下约束：在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序

为什么 final 引用不能从构造函数内“溢出”
在构造函数返回前，被构造对象的引用不能为其他线程所见，因为此时的 final 域可能还没有初始化。在构造函数返回后，任意线程都将保证能看到 final 域正确初始化之后的值,如果一个类包含final字段，且在构造函数中初始化，那么正确的构造一个对象后，final字段被设置后对于其它线程是可见的。

这里所说的正确构造对象，意思是在对象的构造过程中，不允许对该对象进行引用，不然的话，可能存在其它线程在对象还没构造完成时就对该对象进行访问，造成不必要的麻烦。

```
class FinalTest{
    final int x;
    int y ;
    static finalTest f;
    FinalTest(){
        x =3;
        y =4;
    }
    
    static void writer(){
        f = new FinalTest();
    }
    
    static void reader(){
        if(f!=null){
            int i = f.x;
            int j = f.y;
        }
    }
}


```

上面这个例子描述了应该如何使用final字段，一个线程A执行reader方法，如果f已经在线程B初始化好，那么可以确保线程A看到x值是3，因为它是final修饰的，而不能确保看到y的值是4。


### 单线程as-if-serial语义
as-if-serial语义的意思是：==不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变==。编译器、runtime和处理器都必须遵守as-if-serial语义。


```
double pi = 3.14;     //A
double r  = 1.0;       //B
double area = pi * r * r;     //C
```
如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序：

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsd084652f723806fbda25f75393e5a3e6.jpg)


不过，因为as-if-serial语义存在，使程序员不必担心单线程中重排序的问题干扰他们，也无需担心内存可见性问题。但是重排序虽然不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子：

```
class Demo {
  int a = 0;
  boolean flag = false;

  public void write() {
     a = 1;            //1
    flag = true;    //2
   }

  public void read() {
    if(flag) {            //3
      int i = a * a;    //4
    }
  }
}
```
由于操作 1 和 2 没有数据依赖关系，编译器和处理器可以对这两个操作重排序；操作 3 和操作 4 没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsf890b1c0c697d83f4e03a451641727ff.jpg)

如上图所示，操作 1 和操作 2 做了重排序。程序执行时，线程 A 首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！
2、当操作 3 和操作 4 重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsd13b7478b0cdaef83120dc525f3cb1f8.jpg)
在程序中，操作 3 和操作 4 存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a * a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作 3 的条件判断为真时，就把该计算结果写入变量 i 中。

从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！

在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。

所以：
- 重排序可能导致线程安全问题
- 重排序不是必然出现的
- 指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。



### 顺序一致性

顺序一致性内存模型有两大特性：
- 一个线程中的所有操作必须按照程序的顺序来执行。
- （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgsa78178ddd30d93929388fa0a1637ccb5.jpg)

在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。

>假设有两个线程 A 和 B 并发执行。其中 A 线程有三个操作，它们在程序中的顺序是：A1 -> A2 -> A3。B 线程也有三个操作，它们在程序中的顺序是：B1 -> B2 -> B3。假设这两个线程使用监视器锁来正确同步：A 线程的三个操作执行后释放监视器锁，随后 B 线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图所示：![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs48fb5f242e8634eeb798c089b6617805.jpg)现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图：![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs3398173ca6f01a1281958b642095c916.jpg)未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程 A 和 B 看到的执行顺序都是：B1 -> A1 -> A2 -> B2 -> A3 -> B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。

但是，在 JMM 中就没有这个保证。未同步程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。



### happens-before原则
（JMM定义happens-before原则，让我们了解了JVM本身提供的关于并发安全(主要是有序性和可见性)的保障规则。）


JMM与可见性：==依赖于内存屏障，通过禁止某些重排序，可供内存可见性保障，也实现了Happen-Before规则。==

Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。

JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证（如果A线程的写操作a与B线程的读操作b之间存在happens-before关系，尽管a操作和b操作在不同的线程中执行，但JMM向程序员保证a操作将对b操作可见）具体的定义为：

1. 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。
2. 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。


下面就来具体介绍下happens-before原则 ==（先行发生原则）==：
1. 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作（因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。）
1. 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作
1. volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作
1. 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C
1. 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作
1. 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生
1. 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行
1. 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始
# 原子性：
原子性：==不可分割，操作在其他线程看来不可以分割==。一般来说锁通过互斥来保障原子性，互斥是指一个锁一次只能被一个线程所持有，所以，临界区代码只能被一个线程执行，即保障了原子性。提供互斥访问，==同一时刻只能有一个线程来对他进行访问==.

```
int a = 2;
a++;//不是原子性，因为a++涉及三个操作：1.读取变量a的值；2.对a加一；3.将新值赋值给变量a
```

对于涉及到共享变量访问的操作，若该操作从执行线程以外的任意线程来看是不可分割的，那么该操作就是原子操作，该操作具有原子性.即，其它线程不会“看到”该操作执行了部分的中间结果

Java中实现原子性的两种操作：
- 锁（Lock）
- CAS（Compare-and-Swap）指令，俗称硬件锁

实现原子性：
- volatile关键字：仅仅能保证变量写操作的原子性，==不能保证读写操作的原子性==
- sychronized: 依赖JVM，不可中断锁，适合竞争不激烈，可读性好
- Lock:依赖特殊的CPU指令，代码实现ReentrantLock，可中断锁，多样化同步，竞争激烈时能维持常态。
- Atomic：竞争激烈时能维持常态，性能比lock好，但只能同步一个值

# 有序性：
即程序执行的顺序按照代码的先后顺序执行。

写线程在临界区中所执行的一系列操作在读线程所执行的临界区看起来像是完全按照源代码顺序执行的。即：一个线程对著内存的修改可以及时被其他线程观察到：sychronized/volatile/lock。

```
int i = 0;              
boolean flag = false;

i = 1;                //语句1  
flag = true;          //语句2

//结果可能 执行2再执行1 ，也可能先执行1再执行2，因为重排序
```

```
int a = 2;//1
int b = 2+a;//2

//这个时候就只会先执行1再执行2 ，因为数据依赖性b依赖a，那么a先执行
```




# 可见性：
1. 多线程环境下，==一个线程对于某个共享变量的更新，后续访问该变量的线程可能无法立刻读取到这个更新的结果，这就是不可见的情况==。

1. 可见性就是指一个线程对共享变量的更新的结果对于读取相应共享变量的线程而言是否可见的问题
    - 获得锁之后，需要刷新处理器缓存，使得前面写线程所做的更新可以同步到本线程
    - 释放锁需要冲刷处理器缓存，使得当前线程对共享数据的改变可以被推送到下一个线程处理器的高速缓冲中。

1. 可见性和原子性的联系和区别：
    - 原子性描述的是一个线程对共享变量的更新，从另一个线程的角度来看，它要么完成，要么尚未发生。
    - 可见性描述一个线程对共享变量的更新对于另一个线程而言是否可见



```
volatile int a = 2;

//线程1
a = 10;

//线程2
a = a+15;

//线程1和线程2之间可见（1先执行2后执行），a先执行a=10刷新缓存；线程2知道后执行a+15= 25
```


>一个线程观察其他线程中的指令执行顺序，由于指令重排序的存在，该观察结果一般杂乱无序.

导致共享变量不可见原因：
1. 线程交叉执行，
2. 重排序结合线程交叉排序，
3. 共享变量更新后的值没有在工作内存与著内存间更新。

实现可见性:

- sychronized：1、线程解锁前，必须把共享变量的最新值刷新到主内存。2、线程加锁时候，清空工作内存中共享变量的值，从而使用共享变量时需要从主内存重新读取最新的值。

- volatile: 通过加入内存屏障和禁止重排序优化来实现。1、对变量写操作时候加入store屏障指令，将本地内存中共享变量刷新到主内存。2、对变量进行读操作时候，会在读操作前加入一条load屏障指令，从主内存读取共享变量。


#　同步机制与共享内存
>同步机制应该遵循的基本准则 
- 空闲让进：当无进程处于临界区时，表明临界资源处于空闲状态，允许一个请求进入临界区的进程立即进入临界区，以有效利用临界资源
-  忙则等待：当已有进程处于临界区时，表明临界资源正在被访问，因而其他试图进入临界区的进程必须等待，以保证对临界资源的互斥访问
-  有限等待：对要求访问临界资源的进程，应保证在有限时间内能进入自己的临界区，以免陷入“死等”状态
-  让权等待：当进程不能进入自己的临界区时，应立即释放处理机，以免进程陷入“忙等”状态


>共享内存并未提供同步机制，当某一个进程对共享内存提供写操作时，并未自动的阻止另一个进程对它进行读取；
- 共享内存：就是允许多个进程访问同一内存空间，进程间传递和共享数据非常有效
- 信号量：为了防止多个进程访问共享资源而造成冲突，设置临界区域每次只让一个进程访问，信号量提供了这样一种机制，用来控制对临界区域的访问